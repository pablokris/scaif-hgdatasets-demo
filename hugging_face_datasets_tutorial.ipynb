{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd7s3HLgrtQu0PBsyaS+Cy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablokris/scaif-hgdatasets-demo/blob/main/hugging_face_datasets_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hugging Face Datasets**\n",
        "\n",
        "Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.\n",
        "\n",
        "Load a dataset in a single line of code, and use our powerful data processing methods to quickly get your dataset ready for training in a deep learning model. Backed by the Apache Arrow format, process large datasets with zero-copy reads without any memory constraints for optimal speed and efficiency.\n",
        "\n",
        "Let' start by pip installing the module\n"
      ],
      "metadata": {
        "id": "OxZYi_Z-iWgn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGBK2dBFhElL"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading a Dataset**\n",
        "\n",
        "Loading a dataset is as easy as one line from the [hugging face datasets hub](https://huggingface.co/?activityType=update-dataset&feedType=following)\n",
        "\n",
        "We'll talk about splitting the next section\n"
      ],
      "metadata": {
        "id": "xI0H9X2qjfC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")"
      ],
      "metadata": {
        "id": "ULACyGV6hK23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Splitting**\n",
        "\n",
        "Splitting refers to how models are often split (partitioned) to make subsets of data so you don't have to download all of the data for testing, training, ect. If you do not configure this attribute it usually returns the all the data. Splits are often defined as \"train\", \"test\", \"validation\"]\n"
      ],
      "metadata": {
        "id": "ovw4nDz0kqg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_split_names\n",
        "\n",
        "get_dataset_split_names(\"rotten_tomatoes\")"
      ],
      "metadata": {
        "id": "i1jPxP97hgTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can load a specific split with the split parameter. Loading a dataset split returns a Dataset object"
      ],
      "metadata": {
        "id": "dnGj2yjFmTF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "KytsZe8Shl7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you donâ€™t specify a split, Datasets returns a DatasetDict object instead"
      ],
      "metadata": {
        "id": "GR1egSCJmA6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rotten_tomatoes\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "YxB4a6e4hq8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurations**\n",
        "Some datasets contain several sub-datasets. For example, the MInDS-14 dataset has several sub-datasets, each one containing audio data in a different language. These sub-datasets are known as configurations, and you must explicitly select one when loading the dataset. If you donâ€™t provide a configuration name, ðŸ¤— Datasets will raise a ValueError and remind you to choose a configuration.\n",
        "\n",
        "Use the get_dataset_config_names() function to retrieve a list of all the possible configurations available to your dataset:"
      ],
      "metadata": {
        "id": "wpYg-KTrmh7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "\n",
        "configs = get_dataset_config_names(\"PolyAI/minds14\")\n",
        "print(configs)"
      ],
      "metadata": {
        "id": "v9PNcoLah2a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then load the configuration you want"
      ],
      "metadata": {
        "id": "9Gds__Ojmv86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")"
      ],
      "metadata": {
        "id": "d_Mec0mHh71u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Know your dataset**\n",
        "\n",
        "A Dataset contains columns of data, and each column can be a different type of data. The index, or axis label, is used to access examples from the dataset."
      ],
      "metadata": {
        "id": "PSynVbBenUAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
        "dataset[0]"
      ],
      "metadata": {
        "id": "XVFHEt3ynN2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[-1]"
      ],
      "metadata": {
        "id": "tOlPnD8zoLW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing by the column name returns a list of all the values in the column"
      ],
      "metadata": {
        "id": "M7GNSQdNpA1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"text\"]"
      ],
      "metadata": {
        "id": "WXeVUVtSpCdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can combine row and column name indexing to return a specific value at a position"
      ],
      "metadata": {
        "id": "nDs1u4PcpLC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"text\"]"
      ],
      "metadata": {
        "id": "xKCBwuiUpMev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Slicing**\n",
        "Slicing returns a slice - or subset - of the dataset, which is useful for viewing several rows at once"
      ],
      "metadata": {
        "id": "oal69zDUpVHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:3] # Get the first three rows"
      ],
      "metadata": {
        "id": "FlRzmGV8pWAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[3:6] # Get rows between three and six\n"
      ],
      "metadata": {
        "id": "_G3XkenQpeL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize your Text\n",
        "\n",
        "*   Models cannot process raw text, so youâ€™ll need to convert the text into numbers.\n",
        "*   Tokenization provides a way to do this by dividing text into individual words called tokens.\n",
        "*   Tokens are finally converted to numbers."
      ],
      "metadata": {
        "id": "2pZ4ZaE_6PoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "u9kCfbB26RLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")"
      ],
      "metadata": {
        "id": "_qadnK266vfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer returns a dictionary with three items:\n",
        "\n",
        "input_ids: the numbers representing the tokens in the text.\n",
        "token_type_ids: indicates which sequence a token belongs to if there is more than one sequence.\n",
        "attention_mask: indicates whether a token should be masked or not."
      ],
      "metadata": {
        "id": "0lbGLycZ7C5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(dataset[0][\"text\"])"
      ],
      "metadata": {
        "id": "J9P_esHC66Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fastest way to tokenize your entire dataset is to use the map() function. This function speeds up tokenization by applying the tokenizer to batches of examples instead of individual examples. Set the batched parameter to True"
      ],
      "metadata": {
        "id": "tWa0H_G77VEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(example):\n",
        "    return tokenizer(example[\"text\"])\n",
        "\n",
        "dataset = dataset.map(tokenization, batched=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "2PeFxq3c7V1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the format of your dataset to be compatible with your machine learning framework:"
      ],
      "metadata": {
        "id": "cxOhg9zx7rJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
        "dataset.format['type']"
      ],
      "metadata": {
        "id": "Nbd87KOP7uH8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}